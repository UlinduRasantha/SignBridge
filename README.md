ğŸ“˜ README.md
# ğŸ§â€â™‚ï¸ SignBridge â€” Real-Time Sri Lankan Sign Language (SLSL) Translator App  

**SignBridge** is an AI-powered two-way translation mobile application enabling hybrid communication between hearing-impaired and hearing individuals.  
It supports **real-time Sign-to-Text** via gesture recognition and **Text-to-Sign** via a 3D animated avatar â€” providing a seamless and intuitive communication bridge.  

---

## ğŸ§­ Overview

- First-of-its-kind mobile app tailored for Sri Lankan Sign Language (SLSL).  
- Converts live hand gestures into readable text.  
- Converts typed text to animated SLSL gestures through a 3D avatar.  
- Designed for accessibility, independence, and inclusive communication.  

---

## âœ¨ Key Features

- **Real-time Gesture Recognition** â€” Uses hand and pose landmark detection for accurate sign classification.  
- **AI-Powered Inference** â€” Machine learning model (LSTM) to predict sign language gestures reliably.  
- **3D Avatar Rendering** â€” Animated 3D avatar for clear Text-to-Sign translation.  
- **Cross-Platform Mobile App** â€” Built with Flutter for wide device compatibility.  
- **Support for Static & Dynamic Signs** â€” Covers static alphabet gestures and dynamic multi-frame signs.  
- **Extensible Design** â€” Architecture allows scaling to phrase-level translation, other regional sign languages, and future enhancements.  

---

## ğŸ› ï¸ Tech Stack

| Layer | Technologies |
|-------|--------------|
| **Mobile App** | Flutter, Dart |
| **Backend / ML Server** | Python, MediaPipe, TensorFlow / LSTM, Flask API |
| **3D Animation & Avatar** | Blender (modeling & rigging), 3D asset export |
| **Data Processing** | NumPy, OpenCV, custom preprocessing pipelines |
| **Communication** | HTTP / WebSocket API for real-time inference & response |

---

## ğŸ— System Architecture



[ Smartphone Camera ]
â†“ Capture Hand & Pose Data
[ MediaPipe ] â†’ Landmark Extraction
â†“
Preprocessing â†’ Normalization
â†“
LSTM Model â†’ Gesture Prediction
â†“
Flask API â†’ Send Prediction to Mobile Client
â†“
Flutter App â€” Display text or trigger avatar animation
â†“
Animated 3D Avatar shows SLSL sign for Text-to-Sign flow


---

## ğŸ“ Project Structure



signbridge/
â”œâ”€â”€ backend/ # ML model, preprocessing, Flask API
â”‚ â”œâ”€â”€ models/ # Trained LSTM model & checkpoints
â”‚ â”œâ”€â”€ preprocessing/ # Landmark extraction & data cleaning
â”‚ â”œâ”€â”€ api.py # REST / WebSocket endpoints
â”‚ â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mobile_app/ # Flutter project
â”‚ â”œâ”€â”€ lib/ # App source code
â”‚ â”œâ”€â”€ assets/ # Avatar assets, configuration
â”‚ â””â”€â”€ pubspec.yaml
â”‚
â”œâ”€â”€ avatar/ # 3D model & animations (Blender files / exports)
â”‚ â””â”€â”€ <.blend, .glb, .json animation data>
â”‚
â””â”€â”€ README.md # Project documentation (this file)


---

## ğŸ”„ Figma Prototype â€” UI / UX Design  

<iframe  
  src="https://embed.figma.com/design/7mUHZcsbdboePbW8VL9qIq?embed-host=signbridge"  
  style="border:1px solid rgba(0,0,0,0.1);"  
  width="100%"  
  height="600px"  
  allowfullscreen>  
</iframe>

You can interact with the full prototype and test UI flows directly.  
If embed fails (due to GitHub restrictions), use this link:  
[View the SignBridge Figma Prototype â†’](https://www.figma.com/design/7mUHZcsbdboePbW8VL9qIq?node-id=0-1)

> â„¹ï¸ Note: For public visibility of embedded prototypes, ensure the Figma file sharing settings are set to â€œAnyone with link can view.â€ :contentReference[oaicite:2]{index=2}

---

## ğŸš€ How to Run / Setup  

### 1. Backend (Gesture Recognition API)  
```bash
cd backend
pip install -r requirements.txt
python api.py         # Starts Flask server for real-time inference

2. Flutter Mobile App
cd mobile_app
flutter pub get
flutter run           # Runs the app on connected device / emulator

3. Avatar Animations

Ensure the 3D model and exported animation assets reside under mobile_app/assets/ (or declared asset path).
No extra build steps required â€” Flutter will load the animation files as configured.

ğŸ“¸ Screenshots / Demo

(Add your actual app screenshots / demo GIFs below as files in /screenshots and link them accordingly)

Home Screen	Sign-to-Text Camera	Text-to-Sign Avatar

	
	
âœ… Achievements & Impact

First SLSL translator app combining real-time recognition + avatar-based reverse translation.

Demonstrated success in departmental demo/testing.

Provides accessible communication tools for hearing-impaired community in Sri Lanka.

ğŸ”® Future Enhancements

Expand gesture vocabulary â€” support full sentences and conversations.

Integrate phrase-level grammar processing for natural translation.

Optimize model for on-device inference to eliminate network dependency.

Support Sinhala / Tamil / English multilingual UI & translation.

Enhance avatar realism with lip sync / facial expression corresponding to grammar.

ğŸ‘¥ Team â€” Group 36
Name	Role / Contribution
P.B.B. Balasuriya (22UG1-0463)	Dataset & ML model creation, backend integration
U.R. Samarappuli (22UG1-0465)	MediaPipe processing, gesture recording, data preprocessing
U.S. Shashika (22UG1-0495)	Flutter app development, UI/UX, avatar integration, demo prep

Supervisor: Ms. Nilupuli Ekanayake

ğŸªª License

This project is released under the MIT License.
See the LICENSE file for details.
